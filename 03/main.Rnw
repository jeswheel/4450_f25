\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setcounter{tocdepth}{2}}
\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{3}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Joint Distributions}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]

<<setup,include=FALSE,cache=FALSE,purl=FALSE,child="../setup.Rnw">>=
@

\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Introduction}

\begin{frame}{Introduction}
  \begin{itemize}
    \item This material is based on the textbook by \citet[][Chapter~3]{rice07}.
    \item Our goal is to better understand the joint probability structure of more than one random variable, defined on the same sample space.
    \item One reason that studying joint probabilities is an important topic is that it enables us to use what we know about one variable to study another.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Joint cdf}
  \begin{itemize}
    \item Just like the univariate case, the joint behavior of two random variables, $X$ and $Y$, is determined by the cumulative distribution function
    $$
    F(x, y) = P(X \leq x, Y \leq y).
    $$
    \item This is true for both discrete and continuous random variables.
    \item The any set $A \subset \R^2$, the joint cdf can give $P\big((X, Y) \in A\big)$.
  \end{itemize}
  
  \framebreak

  \begin{itemize}
    \item For example, let $A$ be the rectangle defined by $x_1 < X < x_2$, and $y_1 < Y < y_2$. (It helps to draw a picture...)
    \item $F(x_2, y_2)$ gives $P(X < x_2, Y < y_2)$, an area that is too big, so we subtract off pieces
    \begin{itemize}
      \item $F(x_2, y_1) = P(X < x_2, Y < y_1)$ (we already have the area $X < x_2$, but now subtract away the area $Y < y_1$).
      \item $F(x_1, y_2) = P(X < x_1, Y < y_2)$ (Now subtracting the area $X < x_1$)
      \item We have ``double subtracted" the area $\{X < x_1, Y < y_1\}$, so we add it back.
    \end{itemize}
    \end{itemize}
  
  $$P\big((X, Y) \in A\big) = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1).$$
  
  \framebreak
  
  \begin{itemize}
    \item The definition also applies to more than two random variables. 
    \item Let $X_1, \ldots, X_n$ be jointly distributed random variables defined on the same sample space. Then
    $$
    F(x_1, x_2, \ldots, x_n) = P(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n).
    $$
    
    \item Like the univariate case, we can also define the pmf and pdf of jointly distributed random variables as well. 
  \end{itemize}
  
\end{frame}

\section{Discrete Random Variables}

\begin{frame}[allowframebreaks]{Discrete Random Variables}

  \begin{block}{Definition: Joint pmf}
    Let $X$ and $Y$ be discrete random variables define on the same sample space, and take on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$, respectively. The \alert{joint pmf} (or joint frequency function), is
    $$
    p(x_i, y_j) = P(X = x_i, Y = y_j).
    $$
  \end{block}

  \begin{itemize}
    \item For discrete RVs, it's often useful to describe the joint pmf as a frequency table.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item Suppose a fair coin is tossed 3 times. Let $X$ denote the number of heads on the first toss, and $Y$ the total number of heads.
    \item The sample space is
    $$
    \Omega = \{hhh, hht, hth, thh, htt, tht, tth, ttt\}.
    $$
    \item The joint pmf can be expressed as the frequency table below (Table~\ref{tab:freq}).
  \end{itemize}

  \begin{table}
  \begin{tabular}{c|cccc}
    & $y$             &             &                 &              \\\hline
  $x$ &           $0$ &           $1$ &           $2$ &          $3$ \\\hline
  $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ &          $0$ \\
  $1$ &           $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
  \end{tabular}
  \caption{\label{tab:freq}Frequency table for $X$ and $Y$, flipping a fair coin three times.}
  \end{table}
  
  \begin{itemize}
    \item Note that the probabilities in Table~\ref{tab:freq} sum to one.
    \item Using the probability laws we have already learned, we can calculate \alert{marginal} probabilities. 
    
    \framebreak
    
    \begin{align*}
      p_Y(0) &= P(Y = 0) \\
      &= P(Y = 0, X = 0) + P(Y = 0, X = 1) \\
      &= \frac{1}{8} + 0 = \frac{1}{8} \\
      p_Y(1) &= P(Y = 1) \\
      &= P(Y = 1, X = 0) + P(Y = 1, X = 1) \\
      &= \frac{2}{8} + \frac{1}{8} = \frac{3}{8}.
    \end{align*}
    
    \framebreak

    \item In general, to find the frequency function for $Y$ and $X$, we just need to sum the appropriate columns or rows, respectively.
    \item $p_X(x) = \sum_i P(x, y_i)$ and $p_Y(y) = \sum_{j} P(x_j, y)$.
    \item The case with multiple random variables is similar:
    $$
    p_{X_i}(x_i) = \sum_{x_j: j\neq i} p(x_1, x_2, \ldots, x_n).
    $$
    \item We can also get marginal frequencies for more than one variable:
    $$
    p_{X_iX_j}(x_i, x_j) = \sum_{x_k: k \notin \{i, j\}} p(x_1, x_2, \ldots, x_n).
    $$
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Multinomial Distribution}
  
  \begin{itemize}
    \item The \alert{multinomial} distribution is a generalization of the binomial distribution.
    \item Suppose there are $n$ independent trials, each with $r$ possible outcomes, with probabilities $p_1, p_2, \ldots, p_r$, respectively.
    \item Let $N_i$ be the total number of outcomes of type $i$ in the $n$ trials, with $i \in \{1, 2, \ldots, r\}$.
    \item The probability of any particular sequence $(N_1, N_2, \ldots, N_r) = (n_1, n_2, \ldots, n_r)$ is
    $$
    p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
    \item The total number of ways to do this was an identity from Chapter~1 (Proposition~1.3):
    $$
    \binom{n}{n_1\, \cdots\, n_r}.
    $$
    \item Combining this gives us the pmf of the multinomial distribution:
  \end{itemize}
  
  \begin{block}{Multinomial Distribution}
    Let $N_1, N_2, \ldots, N_r$ be random variables that follow a multinomial distribution with parameters $N$ and $(p_1, \ldots, p_r)$. The joint pmf is
    $$
    p(n_1, n_2, \ldots, n_r) = \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
  \end{block}
    
    \begin{itemize}
      \item The marginal distribution for any $N_i$ can be found by summing the joint frequency function over the other $n_j$.
      \item While possible, this is a non-trivial algebraic exercise.
      \item The simple alternative is to reframe the problem: Let $N_i$ be the number of successes in $n$ trials, and $\tilde{N}_i = \sum_{j \neq i}N_j$ be the number of failures. The probability of success is still $p_i$, leaving the probability of failure to be $1 - p_i$. 
      \item Thus, we see that the marginal distribution for $N_i$ must follow a binomial distribution:
      \begin{align*}
      p_{N_i}(n_i) &= \sum_{n_j: j \neq i} \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\\
      & = \binom{n}{n_i}p_i^{n_i}(1 - p_i)^{n-n_i}
      \end{align*}
    \end{itemize}
  
\end{frame}

\section{Continuous Random Variables}

\begin{frame}[allowframebreaks]{Continuous Random Variables}
  \begin{itemize}
    \item Let $X, Y$ be continuous random variables with joint cdf $F(x, y)$.
    \item Their \alert{joint density function} is a piecewise continuous function of two variables, $f(x, y)$.
    \item A few properties: 
    \begin{itemize}
      \item $f(x, y) \geq 0$ for all $(x, y) \in \R$ (or the support).
      \item $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x, y) dxdy = 1$.
      \item For any ``measureable set" $A \subset \R^2$, $P\big((X, Y) \in A\big) = \int\int_A f(x, y) dxdy$
      \item In particular, $F(x, y) = \int_{-\infty}^x\int_{-\infty}^y f(u, v)dudv$.
    \end{itemize}
    \item From the fundamental theorem of multivariable calculus, it follows that
    $$
    f(x, y) = \frac{\partial^2}{\partial x\partial y}F(x, y),
    $$
    wherever the derivative is defined.
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Finding joint probabilities}
    Let $X, Y$ be jointly defined RVs with pdf
    $$
    f(x, y) = \frac{12}{7}(x^2 + xy), \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.
    $$
    Find $P(X > y)$.
    
    \mode<article>{
    \begin{align*} 
      P(X > Y) &= \frac{12}{7}\int_{0}^1\int_{0}^x (x^2 + xy)dydx \\
      &= \frac{9}{14}.
    \end{align*}
    }
    
  \end{exampleblock}
  
  \mode<presentation>{
    \emph{Solution:}
  }
  
  
\end{frame}

\begin{frame}[allowframebreaks]{Marginal cdf}
    The \alert{marginal cdf} of $X$, denoted $F_X$, is
    \begin{align*}
      F_X(x) &= P(X \leq x) \\
      &= P(X \leq x \cap Y \in \R) = P(X \leq x \cap Y < \infty)\\
      &= \lim_{y \rightarrow \infty} F(x, y) \\
      &= \int_{-\infty}^x\int_{-\infty}^{\infty} f(u, y)dydu.
    \end{align*}
    By taking the derivative of both sides of the equation, we get the \alert{marginal density} of $X$:
    $$
    f_X(x) = F_{X}'(x) = \int_{-\infty}^\infty f(x, y)dy.
    $$
    
    \framebreak
    
    \begin{exampleblock}{Calculating Marginal Densities}
      Using the same joint distribution as the previous example, find the marginal density of $X$: 
      \begin{align*}
      f_X(x) &= \int_Y f(x, y) dy \\
      &= \frac{12}{7} \int_{0}^1 (x^2 + xy)dy \\
      &= \frac{12}{7} \Big(x^2y + \frac{x}{2}y^2\Big)\Big|^{1}_{0} \\
      &= \frac{12}{7} \Big(x^2 + \frac{x}{2}\Big)
      \end{align*}
    \end{exampleblock}
\end{frame}

\begin{frame}{More than two random variables}
  \begin{itemize}
    \item For several jointly continuous random variables, we can make the obvious generalizations.
    \item That is, to find the \emph{marginal} densities, we need to ``marginalize-" or ``integrate-" out the \alert{nusaince} variables.
    \item This means integrating out any combination of variables that we want.
    \item Example: Let $X$, $Y$, and $Z$ be jointly continuous RVs with pdf $f(x, y, z)$. Then the two-dimensional marginal distribution of $X$ and $Z$ is: 
    $$
    f_{XZ}(x, z) = \int_{-\infty}^\infty f(x, y, z) dy.
    $$
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Example: constructing bivariate cdfs}
  \begin{itemize}
    \item Suppose that $F(x)$ and $G(y)$ are cdfs for random variables $X$ and $Y$, resp. 
    \item It can be shown that the following function, $H(x, y)$, is always a bivariate cdf for all $-1 \leq \alpha \leq 1$:
    $$
    H(x, y) = F(x)G(y)\Big(1 + \alpha \big(1 - F(x)\big)\big(1 - G(y)\big)\Big).
    $$
    \item Because $\lim_{x \rightarrow \infty} F(x) = \lim_{y \rightarrow \infty} G(x) = 1$, the marginal distributions are:
    \begin{align*}
      \lim_{y \rightarrow \infty}H(x, y) &= F(x) \\
      \lim_{x \rightarrow \infty}H(x, y) &= G(y)
    \end{align*}
    \item Thus, we can use this approach to build an infinite number of biviariate distributions that have a particular marginal distribution.
    
    \framebreak
    
    \item One important example is when the marginal distributions are uniformly distributed.
    \item Let $F(x) = x, 0 \leq x \leq 1$, and $G(y) = y, 0 \leq y \leq 1$.
    \item By selecting $\alpha = -1$, we have 
      \begin{align*}
        H(x, y) &= xy[1 - (1-x)(1-y)]\\
        &= x^2y + y^2x - x^2y^2, \quad 0\leq x, y \leq 1.
      \end{align*}
    \item The density is
      \begin{align*}
        h(x, y) &= \frac{\partial^2}{\partial x\partial y}H(x, y) \\
        &= 2x + 2y - 4xy, \quad 0 \leq x, y \leq 1.
      \end{align*}
    \item \href{https://www.desmos.com/3d/p0makz897t}{Here is a link} to a 3D rendering of this function.

  \framebreak
  
    \item Now, let's select $\alpha = 1/2$:
    \begin{align*}
      H(x, y) &= xy\Big(1 + \frac{1}{2}\big(1 - F(x)\big)\big(1 - G(y)\big)\Big) \\
      &= \frac{1}{2}x^2y^2 - \frac{1}{2}x^2y - \frac{1}{2}xy^2 + \frac{3}{2}xy.
    \end{align*}
    \item Taking the derivative, we get:
    \begin{align*}
      h(x, y) &= \frac{\partial^2}{\partial x\partial y}H(x, y) \\
      &= 2xy - x - y +\frac{3}{2}, \quad 0 \leq x, y \leq 1.
    \end{align*}
    \item \href{https://www.desmos.com/3d/otohcn69lx}{Here is a link} to a 3D rendering of this function.
  
  \framebreak
  
    \item The last two joint cdfs were examples of a \alert{copula}. 
    
    \begin{block}{Definition: Copulas}
      A copula is a joint cdf that has uniform marginal distributions.
    \end{block}
    
    \item Let $C(u, v)$ be a copula. One immediate consequence of the definition is that if $U$ and $V$ are uniform random variables, then $P(U \leq u) = C(u, 1) = u$, and $P(V \leq v) = C(1, v) = v$. 
    
    \framebreak
    
    \item Let $C(u, v)$ be a copula, we will restrict ourselves to the case where it is twice differentiable, such that $c(u,v) = \frac{\partial^2}{\partial u\partial v} C(u, v) \geq 0$.
    \item let $F_X$ and $F_Y$ be the cdfs of $X$ and $Y$, resp.
    \item Now define $U = F_X(X)$, and $V = F_Y(Y)$. From Proposition~2.2, $U$ and $V$ are uniformly distributed.
    \item Now consider the function $H(x, y) = C(u, v) = C\big((F_X(x), F_Y(y)\big)$.
    \item Thus, by the property that $C(u, 1) = u$ and $C(1, v) = v$, we have
    \begin{align*}
      C\big(F_X(x), 1\big) &= F_X(x) \\
      C\big(1, F_Y(y)\big) &= F_Y(y).
    \end{align*}
    Therefore by definition, $F_{XY}(x, y) = H(x, y) = C\big((F_X(x), F_Y(y)\big)$.
    \item Using the chain rule, we can differentiate to obtain
    $$
    f_{XY}(x, y) = c\big(F_X(x), F_Y(y)\big)f_X(x)f_Y(y).
    $$
    
    \framebreak
    
    \item \alert{Takeaway:} We took arbitrary marginal distributions $F_X$ and $F_Y$, and created a family of joint density functions, defined by \emph{any} copula. Thus: the marginal distributions do not determine the joint distribution.
    \item There is a Theorem known as Sklar's Theorem \citep{wikiSklar} that generalizes this statement: All joint distributions can be expressed using a copula and marginal distributions, \emph{and} the representation is unique.
    \item That is, the copula can be thought of as a way to describe the dependence between the variables in any joint distribution. 
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Uniform on specific region}

\begin{itemize}
  \item So far when we have talked about \emph{uniform distributions}, we think about being uniform over $[0, 1]$, or a higher dimensional box: $[a, b]^d$.
  \item It's often useful to have a uniform distribution for other regions of space.
  \item Let $R\subset \R^2$ be any region of interest. The two-dimensional uniform distribution over $R$ is defined by the probability
  $$
  P\big((X, Y) \in A\big) = \frac{|A|}{|R|},
  $$
  where $|\,|$ denotes the measure of the area.
\end{itemize}

\framebreak

\begin{itemize}
  \item Example: Suppose a point is chosen randomly in a disk of radius $1$.
  \item The area of the disk is $\pi r^2 = \pi$, and therefore the joint pdf for the location $(X, Y)$ is
  $$
  f(x, y) = \begin{cases} \frac{1}{\pi} & x^2 + y^2 \leq 1 \\ 0 & \text{otherwise}\end{cases}
  $$
  \item Now let $R$ be the random variable denoting the distance of the point from the origin.
  \item Note that $R \leq r$ if and only if the point lies in a disk of radius $r$. This disk has area $\pi r^2$, and therefore the joint probability is
  $$
  P(R \leq r) = \frac{\pi r^2}{\pi} = r^2, \quad 0 \leq r \leq 1.
  $$
  \item Taking a derivative, the corresponding density function is 
  $$f_R(r) = 2r, \quad 0 \leq r \leq 1.$$
  
  \framebreak 
  
  \item Now let us compute the marginal density of the $x$ coordinate: 
  
  \mode<article>{
  \begin{align*}
    f_X(x) &= \int_{-\infty}^\infty f(x, y)dy \\
    &= \int_{-\infty}^\infty \frac{1}{\pi}\times 1[x^2 + y^2 \leq 1]dy\\
    &= \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \frac{1}{\pi}dy \\
    &= \frac{2}{\pi} \sqrt{1 - x^2}, \quad -1 \leq x \leq 1.
  \end{align*}
  }
  
\end{itemize}

\end{frame}

\section{Independent Random Variables}

\begin{frame}[allowframebreaks]{Independence}
  \begin{block}{Definition: Independent Random Variables}
    Random variables $X_1, X_2, \ldots, X_n$ are said to be \alert{independent} if their joint cdf factors into the product of their marginal cdf's:
    $$
    F(x_1, x_2, \ldots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2)\cdots F_{X_n}(x_n)
    $$
    for all $x_1, x_2, \ldots, x_n$.
  \end{block}
  
  \begin{itemize}
    \item This definition holds for both continuous and discrete random variables.
    \item For discrete RVs, it is equivalent to state that their joint pmf factors.
    \item For continous RVs, it is equivalent to state that their joint pdf factors.
    
    \framebreak
    
    \item To see why this is true, consider the case of two RVs, $X, Y$.
    \item From the definition, if they are independent, then $F(x, y) = F_X(x)F_Y(y)$.
    \item Taking the second mixed partial derivative makes it immediately clear that the joint pdf $f(x, y)$ factors (assuming all densities exist). 
    \item Conversely, suppose that the densities factor. Then by definition:
    \begin{align*}
      F(x, y) &= \int_{-\infty}^x \int_{-\infty}^y f(u, v) dvdu \\
      &= \int_{-\infty}^x \int_{-\infty}^y f_X(u)f_Y(v) dvdu \\
      &= \Big(\int_{-\infty}^x f_X(u) du\Big)\Big(\int_{-\infty}^y f_Y(v) dv\Big) \\
      &= F_X(x)F_Y(y).
    \end{align*}
    
    \item It can also be shown that the definition implies that if $X$ and $Y$ are independent, then
    $$
    P(X \in A, Y \in B) = P(X \in A)P(X \in B).
    $$
    \item Furthermore, if $g$ and $h$ are functions on $\R$, then $Z = g(X)$ and $W = h(Y)$ are also independent.
  \end{itemize}
  
\end{frame}

\section{Conditional Distributions}

\subsection{Discrete random variables}

\begin{frame}[allowframebreaks]{Conditional distributions: discrete RVs}
  \begin{itemize}
    \item If $X$ and $Y$ are jointly distributed discrete RVs, the \emph{conditional probability} that $X = x_i$ given that $Y = y_i$ is
    \begin{align*}
      P(X = x_i | Y = y_i) &= \frac{P(X = x_i, Y = y_i)}{P(Y = y_i)} \\
      &= \frac{p_{XY}(x_i, y_i)}{P_Y(y_i)},
    \end{align*}
    \item If $p_Y(y_i) = 0$, the probability above is defined to be zero.
    \item We denote this conditional probability as $p_{X|Y}$. 
    \item It's important to note that the conditional pmf is a genuine pmf, as it is non-negative and sums to one.
    \item If $X$ and $Y$ are independent, $p_{Y|X}(y|x) = p_Y(y)$.
  
  \framebreak
  
    \item Let's return to a previous joint pmf example (Table~\ref{tab:freq2}).
  
  \end{itemize}

  \begin{table}
  \begin{tabular}{c|cccc}
    & $y$             &             &                 &              \\\hline
  $x$ &           $0$ &           $1$ &           $2$ &          $3$ \\\hline
  $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ &          $0$ \\
  $1$ &           $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
  \end{tabular}
  \caption{\label{tab:freq2}Frequency table for $X$ and $Y$, flipping a fair coin three times.}
  \end{table}
  
  \begin{itemize}
    \item The conditional frequency function of $X$ given $Y = 1$ is:
    \begin{align*}
      p_{X|Y}(0|1) &= \frac{2/8}{3/8} = 2/3 \\
      p_{X|Y}(1|1) &= \frac{1/8}{3/8} = 1/3
    \end{align*}
  \end{itemize}
 
 \framebreak
 
 \begin{itemize}
  \item The definition of the conditional frequency can be reexpressed as
  $$
  p_{XY}(x, y) = p_{X|Y}(x|y)p_Y(y).
  $$
  \item By summing up over all possible values of $y$, we have the following
  $$
  p_X(x) = \sum_y p_{X|Y}(x|y)p_Y(y).
  $$
  \item Both of these identities resemble what we have already seen previously when talking about probabilities: The multiplication principle and the law of total probability.
 \end{itemize}
 
 \framebreak
  
  \begin{exampleblock}{Example: Counting particles}
    Suppose that a particle counter is imperfect; for each particle, it detects the particle with probability $0 < p < 1$. If the number of incoming particles in a unit of time is a Poisson distribution with parameter $\lambda$, what is the distribution of the number of counted particles? 
  \end{exampleblock}
  
  \mode<article>{
  Let $N$ denote the true number of particles, and $X$ the number of counted particles. Because the probability that a particle is counted is independent, then if $N = n$, we have $n$ independent Bernoulli random variables.
  In other words, $X|N = n$ has a binomial distribution with parameters $n$ and $p$. By the law of total probability,
  \begin{align*}
    P(X = k) &= \sum_{n} p_{X|N}(x|n)p_N(n) \\
    &= \sum_{n = k}^\infty \binom{n}{k}p^k(1-p)^{n-k} \frac{\lambda^ne^{-\lambda}}{n!} \\
    &= p^ke^{-\lambda}\sum_{n = k}^\infty \frac{n!}{(n-k)!k!}(1-p)^{n-k} \frac{\lambda^{(n-k)}\lambda^k}{n!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}\sum_{n = k}^\infty \lambda^{n-k}\frac{(1-p)^{n-k}}{(n-k)!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}\sum_{j = 0}^\infty \lambda^{j}\frac{(1-p)^{j}}{j!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}e^{\lambda(1-p)} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda p}
  \end{align*}
  And therefore we see that the distribution of $X$ is a Poisson distribution with parameter $\lambda p$. This is a useful derivation for any situation where events occur following a Poisson process, and then with some probability $p$ and additional, conditional event occurs. For instance, $N$ might be the number of traffic accidents in a given time period, and each accident being fatal or non-fatal with probability $p$. Then $X$ would be the number of fatal accidents.
  }
  
\end{frame}

\subsection{Continuous Random Variables}

\begin{frame}[allowframebreaks]{The continuous case}
  \begin{itemize}
    \item Although a formal argument is beyond the scope of this course, the definition for conditional density of $Y|X$ will be analogous to the discrete case.
  \end{itemize}
  
  \begin{block}{Definition: Conditional density}
    Let $X,Y$ be jointly continuous random variables with joint density $f_{XY}(x, y)$ and marginal densities $f_X(x)$ and $f_Y(y)$, respectively. Then the conditional density of $Y$ given $X$ is defined to be
    $$
    f_{Y|X}(y|x) = \frac{f_{XY}(x, y)}{f_X(x)},
    $$
    if $0 < f_X(x) < \infty$, and $0$ otherwise.
  \end{block}
  
  \framebreak
  
  \begin{itemize}
    \item A heuristic argument of why this definition makes sense is provided in \citet[][Section~3.5.2]{rice07} using differentials. 
    \item With our definition, we can express the joint density in terms of the marginal and conditional densities:
    $$
    f_{XY}(x, y) = f_{Y|X}(y | x)f_X(x).
    $$
    \item We often use this expression to find marginal densities, using principles we have already discussed.
    $$
    f_Y(y) = \int_{\R} f_{XY}(x, y) dx = \int_{\R} f_{Y|X}(y|x)f_X(x)dx.
    $$
    \item We can think of the above expression as the law of total probability for the continuous case.
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: finding conditional densities}
  \begin{itemize}
    \item Let $X$ and $Y$ be jointly distributed random variables with joint and marginal densities
    \begin{align*}
      f_{XY}(x, y) &= \lambda^2 e^{-\lambda y}, \quad 0 \leq x \leq y \\
      f_X(x) &= \lambda e^{-\lambda x}, \quad x\geq 0 \\
      f_Y(y) &= \lambda^2 y e^{-\lambda y}, \quad y \geq 0
    \end{align*}
    \item Note that if $x$ is held constant, the joint density decays exponentially in $y$ for $y \geq x$.
    \item If $y$ is held constant, the joint density is constant for $0 \leq x \leq y$. 
    
    \framebreak
    
    \item Find the conditional densities for $Y|X$ and $X|Y$.
  \end{itemize}
  
  \mode<article>{
  $$
  f_{Y|X}(y | x) = \frac{\lambda^2e^{-\lambda y}}{\lambda e^{-\lambda x}} = \lambda e^{-\lambda(y - x)}, \quad y \geq x.
  $$
  This density follows our intuition of what happens when $x$ is fixed, namely that $y$ appeared to decay exponentially. Now we see that $Y|X$ is exponentially distributed on the interval $[x, \infty)$. Now for $X|Y$,
  $$
  f_{X|Y}(x | y) = \frac{\lambda^2 e^{-\lambda y}}{\lambda^2 y e^{-\lambda y}} = 1/y, \quad 0 \leq x \leq y.
  $$
  Thus the conditional density of $X$ given $Y = y$ is uniform on the interval $[0, y]$.
  }
  
  \framebreak
  
  \begin{itemize}
    \item Suppose we wanted to generate samples from the joint distribution $(X, Y)$; how can this be done?
  
    \item  Using what we have found about the conditional distributions, there are two simple ways for this to be done. Recall that the joint density is $f_{XY}(x, y) = f_{X|Y}(x|y)f_Y(y) = f_{Y|X}(y|x)f_X(x)$.
    \begin{enumerate}
      \item We could generate $X$, which is an exponential random variable $(f_X(x))$. Then, we could generate $Y$ conditioned on the simulated value of $X = x$, which follows an exponential distribution on the interval $[x, \infty)$.
      \item Similarly, we can note that $Y$ has a gamma distribution, and therefore generate a $y$ following a gamma distribution, and then generate a value from $X|Y = y$, which is uniform on $[0, y]$.
    \end{enumerate}
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{The rejection method}

\begin{itemize}
  \item We are often interested in generating random variables from a density function.
  \item If we have a closed form of the inverse cdf, we can use the ``inverse cdf method" (Proposition~2.3).
  \item If a closed-form of the inverse cdf is not available, a commonly used approach is known as \alert{rejection sampling}.
  
  \framebreak
  
  \item Setup: let $f$ be a density function we wish to simulate from, that is non-zero on an interval $[a, b]$.
  \item Pick a function $M(X)$ such that $M(x)\geq f(x)$ on $[a, b]$, and let
  $$
  m(x) = \frac{M(x)}{\int_{a}^{b} M(x)dx}.
  $$
  \item Note that $m(x)$, as defined, is a probability density function. Then, to generate RV with density $f$, we can do the following:
  \begin{itemize}
    \item[Step 1:] Generate $T$ with density $m$.
    \item[Step 2:] Generate $U \sim U[0, 1]$ independent of $T$. \alert{If} $M(T) \times U \leq f(T)$, then we ``accept" $T$ as a sample $(X = T)$; \alert{otherwise}, we ``reject" and go back to Step 1.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Rejection Method Figure}

  \begin{itemize}

  \item A geometric justification is randomly throwing a dart (uniformly) at Figure~\ref{fig:reject}.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth]{reject.png}
    \caption{\label{fig:reject}Illustration of the rejection method, copied from \citet[][Figure~3.15]{rice07}.}
  \end{figure}
  
  \item If the dart lands below the curve $f$, record the $x$ coordinate; otherwise, reject it. With enough throws, the distribution of $x$ coordinates will be proportional to the height of the curve.
  
  \end{itemize}
  
\end{frame}

\begin{frame}{Rejection sampling}
  \begin{itemize}
    \item A more formal argument using differentials is given in \citet[Example~D][Figure~3.15]{rice07}.
    \item In order for the rejection method to be worth-while (computationally efficient), it is important that the algorithm has high-acceptance (good choice of $M$), otherwise you may need a large number of samples because many are being rejected.
  \end{itemize}
\end{frame}

\section{Functions of Jointly Distributed Random Variables}

\begin{frame}[allowframebreaks]{Convolutions}
  \begin{itemize}
    \item Suppose that $X$ and $Y$ are discrete random variables that take values on the integers and joint pmf $p(x, y)$. 
    \item Find the pmf of $Z = X + Y$. 
    
    \item Note that $Z = z$ only when $X = x$ and $Y = z - x$, whenever $x$ is an integer.
    \item Thus, using the law of total probability, we can write 
      $$
      p_Z(z) = \sum_{x = -\infty}^\infty p(x, z-x).
      $$
    \item If $X$ and $Y$ are independent, then $p(x, y) = p_X(x)P_Y(y)$, and
      $$
      p_Z(z) = \sum_{x = -\infty}^\infty p_X(x)p_Y(z-x).
      $$
    \item This sum is called the \alert{convolution} of the sequences $p_X$ and $p_Y$. 
    
    \item The continuous case is similar. Let $X$ and $Y$ be jointly continuous RVs, and $Z = X + Y$.
    \item If we want to find the cdf of $Z$, then:
    \begin{align*}
      F_Z(z) &= P(Z \leq z) \\
             &= P(X + Y \leq z) \\
             &= \int\int_{\{x + y \leq z\}} f(x, y)\, dy\,dx \\
             &= \int_{-\infty}^\infty \int_{-\infty}^{z-x} f(x, y)\, dy\,dx \\
             &= \int_{-\infty}^\infty \int_{-\infty}^{z} f(x, v-x) \, dv\,dx \\
             &= \int_{-\infty}^z \int_{-\infty}^{\infty} f(x, v-x) \, dx\,dv 
    \end{align*}
    \item Differentiating both sides, the fundamental theorem of calculus (with proper assumptions) gives
    $$
    f_Z(z) = \int_{-\infty}^\infty f(x, z-x) dx
    $$
    
    \item Like in the discrete case, if $X$ and $Y$ are independent, then
    $$
    f_Z(z) = \int_{-\infty}^\infty f_X(x)f_Y(z-x) dx
    $$
    \item This integral is called the \alert{convolution} of the functions $f_X$ and $f_Y$. 
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Example: Sum of Exponential RVs}
    Suppose that the lifetime of an electrical component is exponentially distributed with rate $\lambda$, and that and independent and identical backup is available. If the system operates as long as one of the components is functional, and the components will not be replaced if they fail, what is the distribution of the life of the system?
  \end{exampleblock}

  \emph{Solution:}
  
  \mode<article>{
  \vspace{2mm}
    Let $T_1$ and $T_2$ denote the lifetimes of the two component, respectively. The lifetime of the system is $X = T_1 + T_2$. Thus, the pdf of $X$ can be calculated as the convolution:
    \begin{align*}
    f_X(x) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)dx \\
    &= \int_{0}^x \lambda e^{-\lambda t} \times \lambda e^{-\lambda (x - t)}dt \\
    &= \lambda^2 \int_{0}^x e^{-\lambda x}dt \\
    &= \lambda^2 x e^{-\lambda x}, \quad x \geq 0.
    \end{align*}
  }

  \framebreak

  \begin{itemize}
    \item In the previous example, note carefully the change in integration:
    \begin{itemize}
      \item The exponential density is only positive when $t > 0$, and zero every where else.
      \item Thus, from $(-\infty, 0)$, the integral is zero.
      \item Similarly, we evaluate the density at $x - t$, and hence when $t > x$, the integral is also zero.
    \end{itemize}
    \item You may notice that the density of $X = T_1 + T_2$ that we calculated is the same as a gamma distribution with parameters $2$ and $\lambda$.
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Quotients of random variables}
  \begin{itemize}
    \item Let $X$ and $Y$ be jointly continuous random variables, and let $Z = Y/X$.
    \item Our derivation for the pdf of $Z$ is similar as what we did with the sum: find the cdf, then take the derivative.
    \item $F_Z(z) = P(Z \leq z) = P(Y/X \leq z)$. Thus, we are interested in the probability of the set $\{x, y: y/x \leq z\}$.
    \item We have to be a little careful about what happens if $X = 0$, so we will split it into two parts: 
    \begin{itemize}
      \item If $x > 0$, then the set is $y \leq xz$.
      \item If $x < 0$, then the set is $y \geq xz$. 
    \end{itemize}
    Thus,
    $$
      F_Z(z) = \int_{-\infty}^0 \int_{xz}^\infty f(x, y)\, dy\, dx + \int_{0}^{\infty} \int_{-\infty}^{xz} f(x, y)\, dy\,dx.
    $$
    \item To remove dependence of the inner integrals on $x$, we make the change of variables $y = xv$:
    \begin{align*}
      F_Z(z) &= \int_{-\infty}^0 \int_{z}^{-\infty} xf(x, xv)\, dv\, dx + \int_{0}^{\infty} \int_{-\infty}^{z} xf(x, xv)\, dv\,dx \\
      &= \int_{-\infty}^0 \int_{-\infty}^{z} (-x)f(x, xv)\, dv\, dx + \int_{0}^{\infty} \int_{-\infty}^{z} xf(x, xv)\, dv\,dx \\
      &= \int_{-\infty}^z \int_{-\infty}^\infty |x| f(x, xv)\, dx\, dv 
    \end{align*}
    \item And differentiating both sides, we obtain
    $$
    f_Z(z) = \int_{-\infty}^\infty |x| f(x, xz)\, dx.
    $$
    \item If $X$ and $Y$ are independent, 
    $$
    f_Z(z) = \int_{-\infty}^{\infty} |x|f_X(x)f_Y(xz) dx.
    $$
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Example: Cauchy density}

\begin{itemize}
  \item Let $X$ and $Y$ be independent, standard normal random variables.
  \item We wish to find the pdf of $Z = Y / X$.
  \item Using the expression we previously derived for the quotient of independent RVs, we have
  $$
  f_Z(z) = \int_{-\infty}^{\infty} \frac{|x|}{2\pi}e^{-x^2/2}e^{-x^2z^2/2} \, dx.
  $$
  \item Because the integrand is symmetric, we can re-express this as
  \begin{align*}
    f_Z(z) &= 2\int_{0}^{\infty} \frac{|x|}{2\pi}e^{-x^2/2}e^{-x^2z^2/2} \,dx \\
     &= \frac{1}{\pi}\int_{0}^\infty x e^{-x^2\big((z^2 + 1)/2\big)} \, dx \\
     &= \frac{1}{2\pi} \int_{0}^\infty e^{-u\big((z^2 + 1) / 2\big)}\, du \\
     &= \frac{1}{2\lambda \pi} \int_{0}^\infty \lambda e^{-u\lambda}\, du \\
     &= \frac{1}{\pi(z^2 + 1)}, \quad -\infty < z < \infty.
  \end{align*}
  \item Here, I made the substitution $\lambda = (z^2 + 1)/2$, and the integral was calculated using the fact that the pdf of the exponential distribution integrates to one: $\int_0^\infty \lambda e^{-\lambda x} dx = 1$.
  \item This density is called the \alert{Cauchy density}.
  \item Like the standard normal, the Cauchy density is symmetric about zero and bell-shaped, but the tails of the Cauchy tend to zero very slowly.
  \item \href{https://www.desmos.com/calculator/tqvayodexb}{Here is a link} showing this comparison.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{The General Case}
  \begin{itemize}
    \item There is also a way to find the pdf of more general cases, though the derivation is outside the scope of this course.
    \item Let $X$ and $Y$ be jointly distributed, continuous RVs, and suppose we are interested in the joint pdf of $U = g_1(X, Y)$, $V = g_2(X, Y)$, where $g_1$ and $g_2$ are invertible functions with continuous partial derivatives.
    \item We will denote the inverse of $g_1$ and $g_2$ as $X = h_1(U, V)$ and $Y = h_2(U, V)$, respectively.
    % \item The \alert{Jacobian} of the transformation $J(x, y)$ is
    % $$
    % J(x, y) = \det \begin{bmatrix}\frac{\partial g_1}{\partial x} & \frac{\partial g_1}{\partial y} \\ \frac{\partial g_2}{\partial x} & \frac{\partial g_2}{\partial y} \end{bmatrix} = \Big(\frac{\partial g_1}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big) - \Big(\frac{\partial g_2}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big).
    % $$
    \item The pdf of $(U, V)$ can be calculated in two ways:
  \end{itemize}
  
  \begin{block}{Proposition 3.1: Multivariate transformations}
    Under the assumptions above, the joint density of $U$ and $V$ is 
    \begin{align*}
    f_{UV}(u, v) &= f_{XY}\big(h_1(u, v), h_2(u, v)\big)\Big|J_h(u, v)\Big| \\
                 &= f_{XY}\big(x, y\big)\Big|J^{-1}_g\big(x, y\big)\Big|
    \end{align*}
    for $(u, v)$ such that $u = g_1(x, y)$ and $v = g_2(x, y)$ for some $(x, y)$, and $0$ otherwise.
  \end{block}
  
  \begin{itemize}
    \item Above, we call $J_f$ the \alert{Jacobian determinant} (or just Jacobian) of $f$. It is equal to the matrix of partial derivatives: 
    \begin{align*}
      J_h &= \det \begin{bmatrix}\frac{\partial h_1}{\partial u} & \frac{\partial h_1}{\partial v} \\ \frac{\partial h_2}{\partial u} & \frac{\partial h_2}{\partial v} \end{bmatrix} = \Big(\frac{\partial h_1}{\partial u}\Big)\Big(\frac{\partial h_2}{\partial v}\Big) - \Big(\frac{\partial h_2}{\partial u}\Big)\Big(\frac{\partial h_2}{\partial v}\Big) \\
      J_g &= \det \begin{bmatrix}\frac{\partial g_1}{\partial x} & \frac{\partial g_1}{\partial y} \\ \frac{\partial g_2}{\partial x} & \frac{\partial g_2}{\partial y} \end{bmatrix} = \Big(\frac{\partial g_1}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big) - \Big(\frac{\partial g_2}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big)
    \end{align*}
    \item The reason these two expressions are equal is because $J_h = J^{-1}_g$, and we defined $x = h_1(u, v)$ and $y = h_2(u, v)$; you end up with the same result, but sometimes one might be an easier calculation than the other. Both versions are fairly common in textbooks and courses.
    \item I find the first line of the proposition more intuitive: it's a function of $u$ and $v$, so let's start by calculating the Jacobian of the inverse transformation so that all variables are $u$ and $v$. 
    \item The textbook uses the second line approach. You might find this more intuitive, as it is a generalization of the single dimensional case.
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Polar Coordinates}
  \begin{itemize}
    \item Suppose that $X$ and $Y$ are independent standard normal RVs. Their joint pdf is
    $$
    f_{XY}(x, y) = \frac{1}{2\pi}e^{-(x^2/2) - (y^2/2)}.
    $$
    \item We wish the find the joint pdf of $R = \sqrt{X^2 + Y^2}$, and $\Theta = \text{arctan}(y/x)$.
    \item Thus, we have 
    $$
    \begin{cases}
    g_1(x, y) = \sqrt{x^2 + y^2} = r \\
    g_2(x, y) = \text{arctan}(y/x) = \theta,\, \quad \text{if}\, x \neq 0,\, \text{and}\, \theta = 0 \, \text{o.w.}
    \end{cases}
    $$
    \item The inverse transformations are 
    $$
    \begin{cases}
    h_1(r, \theta) = r\cos \theta = x\\
    h_2(r, \theta) = r \sin \theta = y.
    \end{cases}
    $$
    \item The Jacobian of the inverse transformation $J_h$ is
    \begin{align*}
    J_h(r, \theta) &= \det \begin{bmatrix}\frac{\partial h_1}{\partial r} & \frac{\partial h_1}{\partial \theta} \\ \frac{\partial h_2}{\partial r} & \frac{\partial h_2}{\partial \theta} \end{bmatrix} \\
              &= \det \begin{bmatrix} \cos\theta & -r\sin\theta \\ \sin \theta & r\cos \theta  \end{bmatrix} \\
              &= r \cos^2\theta + r \sin^2\theta = r
    \end{align*}
    \item Therefore, the joint distribution is
    \begin{align*}
      f_{R\Theta}(r, \theta) &= r f_{XY}(r \cos\theta, r \sin \theta) \\
      &= \frac{r}{2\pi}e^{-r^2\cos^2\theta / 2 - r^2\sin^2\theta / 2} \\
      &= \frac{r}{2\pi}e^{-r^2/2}.
    \end{align*}
    \item As always, we can't forget the \alert{support}, or values over which the density is positive. Here, because $(X, Y) \in \R^2$, the transformations imply that $\Theta \in [0, 2\pi]$, and $R \geq 0$. 
  \end{itemize}
\end{frame}

% TODO: Include another example? 
% \begin{frame}{Example: Sum of random variables}
%   \begin{itemize}
%     \item Let $X_1$ and $X_2$ be independent, standard normal variables.
%     \item What is the joint density of $(X_1, X_1 + X_2)$?
%     \item Let's let $U = X_1$, and $V = X_1 + X_2$.
%     \item Our transformations are:
%     
%   \end{itemize}
% \end{frame}

\begin{frame}{Transformations of many variables}
\begin{itemize}
  \item Proposition~3.1 can be generalized to transformations of more than two random variables. If $X_1, \ldots, X_n$ have the joint density function $f_{X_1\cdots X_n}$, and 
  \begin{align*}
  Y_i &= g_i(X_1, \ldots, X_n), \quad i = 1, \ldots, n\\
  X_i &= h_i(Y_1, \ldots, Y_n), \quad i = 1, \ldots, n
  \end{align*}
  And if $J_g$ is the determinant of the matrix with the $ij$th entry $\partial g_i/\partial x_j$, and $J_h$ is the determinant of the matrix with entry $\partial h_i / \partial y_j$, then the joint density of $Y_1, \ldots, Y_n$ is
  \begin{align*}
    &f_{Y_1\cdots Y_n}(y_1, \ldots, y_n) \\
    &= f_{X_1\cdots X_n}(x_1, \ldots, x_n)\big|J^{-1}_g(x_1, \ldots, x_n)\big|\\
    &= f_{X_1\cdots X_n}\big(h_1(y_1, \ldots, y_n), \ldots, h_n(y_1, \ldots, y_n)\big)\big|J_h(y_1, \ldots, y_n)\big|
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Final Comments}
  \begin{itemize}
    \item In the transformation formulas, we always transform $n$ variables to $n$ variables. In practice, you might want to consider a transformation from $n \mapsto m$, with $m \leq n$. In this case, there are two main approaches:
    \begin{itemize}
      \item Start from scratch, just like we did for sums and quotients of random variables.
      \item Create dummy variables to make $m = n$ (i.e., $Y_k = X_k$), calculate Jacobian, and then integrate out the dummy variables.
    \end{itemize}
  \end{itemize}
\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version \Sexpr{getRversion()}.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.9]{References and Acknowledgements}

\acknowledgments

\framebreak

\bibliography{../bib4450}

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







