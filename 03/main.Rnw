\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setcounter{tocdepth}{2}}
\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{3}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Joint Distributions}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]

<<setup,include=FALSE,cache=FALSE,purl=FALSE,child="../setup.Rnw">>=
@

\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Introduction}

\begin{frame}{Introduction}
  \begin{itemize}
    \item This material is based on the textbook by \citet[][Chapter~3]{rice07}.
    \item Our goal is to better understand the joint probability structure of more than one random variable, defined on the same sample space.
    \item One reason that studying joint probabilities is an important topic is that it enables us to use what we know about one variable to study another.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Joint cdf}
  \begin{itemize}
    \item Just like the univariate case, the joint behavior of two random variables, $X$ and $Y$, is determined by the cumulative distribution function
    $$
    F(x, y) = P(X \leq x, Y \leq y).
    $$
    \item This is true for both discrete and continuous random variables.
    \item The any set $A \subset \R^2$, the joint cdf can give $P\big((X, Y) \in A\big)$.
  \end{itemize}
  
  \framebreak

  \begin{itemize}
    \item For example, let $A$ be the rectangle defined by $x_1 < X < x_2$, and $y_1 < Y < y_2$. (It helps to draw a picture...)
    \item $F(x_2, y_2)$ gives $P(X < x_2, Y < y_2)$, an area that is too big, so we subtract off pieces
    \begin{itemize}
      \item $F(x_2, y_1) = P(X < x_2, Y < y_1)$ (we already have the area $X < x_2$, but now subtract away the area $Y < y_1$).
      \item $F(x_1, y_2) = P(X < x_1, Y < y_2)$ (Now subtracting the area $X < x_1$)
      \item We have ``double subtracted" the area $\{X < x_1, Y < y_1\}$, so we add it back.
    \end{itemize}
    \end{itemize}
  
  $$P\big((X, Y) \in A\big) = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1).$$
  
  \framebreak
  
  \begin{itemize}
    \item The definition also applies to more than two random variables. 
    \item Let $X_1, \ldots, X_n$ be jointly distributed random variables defined on the same sample space. Then
    $$
    F(x_1, x_2, \ldots, x_n) = P(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n).
    $$
    
    \item Like the univariate case, we can also define the pmf and pdf of jointly distributed random variables as well. 
  \end{itemize}
  
\end{frame}

\section{Discrete Random Variables}

\begin{frame}[allowframebreaks]{Discrete Random Variables}

  \begin{block}{Definition: Joint pmf}
    Let $X$ and $Y$ be discrete random variables define on the same sample space, and take on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$, respectively. The \alert{joint pmf} (or joint frequency function), is
    $$
    p(x_i, y_j) = P(X = x_i, Y = y_j).
    $$
  \end{block}

  \begin{itemize}
    \item For discrete RVs, it's often useful to describe the joint pmf as a frequency table.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item Suppose a fair coin is tossed 3 times. Let $X$ denote the number of heads on the first toss, and $Y$ the total number of heads.
    \item The sample space is
    $$
    \Omega = \{hhh, hht, hth, thh, htt, tht, tth, ttt\}.
    $$
    \item The joint pmf can be expressed as the frequency table below (Table~\ref{tab:freq}).
  \end{itemize}

  \begin{table}
  \begin{tabular}{c|cccc}
    & $y$             &             &                 &              \\\hline
  $x$ &           $0$ &           $1$ &           $2$ &          $3$ \\\hline
  $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ &          $0$ \\
  $1$ &           $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
  \end{tabular}
  \caption{\label{tab:freq}Frequency table for $X$ and $Y$, flipping a fair coin three times.}
  \end{table}
  
  \begin{itemize}
    \item Note that the probabilities in Table~\ref{tab:freq} sum to one.
    \item Using the probability laws we have already learned, we can calculate \alert{marginal} probabilities. 
    
    \framebreak
    
    \begin{align*}
      p_Y(0) &= P(Y = 0) \\
      &= P(Y = 0, X = 0) + P(Y = 0, X = 1) \\
      &= \frac{1}{8} + 0 = \frac{1}{8} \\
      p_Y(1) &= P(Y = 1) \\
      &= P(Y = 1, X = 0) + P(Y = 1, X = 1) \\
      &= \frac{2}{8} + \frac{1}{8} = \frac{3}{8}.
    \end{align*}
    
    \framebreak

    \item In general, to find the frequency function for $Y$ and $X$, we just need to sum the appropriate columns or rows, respectively.
    \item $p_X(x) = \sum_i P(x, y_i)$ and $p_Y(y) = \sum_{j} P(x_j, y)$.
    \item The case with multiple random variables is similar:
    $$
    p_{X_i}(x_i) = \sum_{x_j: j\neq i} p(x_1, x_2, \ldots, x_n).
    $$
    \item We can also get marginal frequencies for more than one variable:
    $$
    p_{X_iX_j}(x_i, x_j) = \sum_{x_k: k \notin \{i, j\}} p(x_1, x_2, \ldots, x_n).
    $$
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Multinomial Distribution}
  
  \begin{itemize}
    \item The \alert{multinomial} distribution is a generalization of the binomial distribution.
    \item Suppose there are $n$ independent trials, each with $r$ possible outcomes, with probabilities $p_1, p_2, \ldots, p_r$, respectively.
    \item Let $N_i$ be the total number of outcomes of type $i$ in the $n$ trials, with $i \in \{1, 2, \ldots, r\}$.
    \item The probability of any particular sequence $(N_1, N_2, \ldots, N_r) = (n_1, n_2, \ldots, n_r)$ is
    $$
    p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
    \item The total number of ways to do this was an identity from Chapter~1 (Proposition~1.3):
    $$
    \binom{n}{n_1\, \cdots\, n_r}.
    $$
    \item Combining this gives us the pmf of the multinomial distribution:
  \end{itemize}
  
  \begin{block}{Multinomial Distribution}
    Let $N_1, N_2, \ldots, N_r$ be random variables that follow a multinomial distribution with parameters $N$ and $(p_1, \ldots, p_r)$. The joint pmf is
    $$
    p(n_1, n_2, \ldots, n_r) = \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
  \end{block}
    
    \begin{itemize}
      \item The marginal distribution for any $N_i$ can be found by summing the joint frequency function over the other $n_j$.
      \item While possible, this is a non-trivial algebraic exercise.
      \item The simple alternative is to reframe the problem: Let $N_i$ be the number of successes in $n$ trials, and $\tilde{N}_i = \sum_{j \neq i}N_j$ be the number of failures. The probability of success is still $p_i$, leaving the probability of failure to be $1 - p_i$. 
      \item Thus, we see that the marginal distribution for $N_i$ must follow a binomial distribution:
      \begin{align*}
      p_{N_i}(n_i) &= \sum_{n_j: j \neq i} \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\\
      & = \binom{n}{n_i}p_i^{n_i}(1 - p_i)^{n-n_i}
      \end{align*}
    \end{itemize}
  
\end{frame}

\section{Continuous Random Variables}

\begin{frame}[allowframebreaks]{Continuous Random Variables}
  \begin{itemize}
    \item Let $X, Y$ be continuous random variables with joint cdf $F(x, y)$.
    \item Their \alert{joint density function} is a piecewise continuous function of two variables, $f(x, y)$.
    \item A few properties: 
    \begin{itemize}
      \item $f(x, y) \geq 0$ for all $(x, y) \in \R$ (or the support).
      \item $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x, y) dxdy = 1$.
      \item For any ``measureable set" $A \subset \R^2$, $P\big((X, Y) \in A\big) = \int\int_A f(x, y) dxdy$
      \item In particular, $F(x, y) = \int_{-\infty}^x\int_{-\infty}^y f(u, v)dudv$.
    \end{itemize}
    \item From the fundamental theorem of multivariable calculus, it follows that
    $$
    f(x, y) = \frac{\partial^2}{\partial x\partial y}F(x, y),
    $$
    wherever the derivative is defined.
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Finding joint probabilities}
    Let $X, Y$ be jointly defined RVs with pdf
    $$
    f(x, y) = \frac{12}{7}(x^2 + xy), \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.
    $$
    Find $P(X > y)$.
    
    \mode<article>{
    \begin{align*} 
      P(X > Y) &= \frac{12}{7}\int_{0}^1\int_{0}^x (x^2 + xy)dydx \\
      &= \frac{9}{14}.
    \end{align*}
    }
    
  \end{exampleblock}
  
  \mode<presentation>{
    \emph{Solution:}
  }
  
  
\end{frame}

\begin{frame}[allowframebreaks]{Marginal cdf}
    The \alert{marginal cdf} of $X$, denoted $F_X$, is
    \begin{align*}
      F_X(x) &= P(X \leq x) \\
      &= P(X \leq x \cap Y \in \R) = P(X \leq x \cap Y < \infty)\\
      &= \lim_{y \rightarrow \infty} F(x, y) \\
      &= \int_{-\infty}^x\int_{-\infty}^{\infty} f(u, y)dydu.
    \end{align*}
    By taking the derivative of both sides of the equation, we get the \alert{marginal density} of $X$:
    $$
    f_X(x) = F_{X}'(x) = \int_{-\infty}^\infty f(x, y)dy.
    $$
    
    \framebreak
    
    \begin{exampleblock}{Calculating Marginal Densities}
      Using the same joint distribution as the previous example, find the marginal density of $X$: 
      \begin{align*}
      f_X(x) &= \int_Y f(x, y) dy \\
      &= \frac{12}{7} \int_{0}^1 (x^2 + xy)dy \\
      &= \frac{12}{7} \Big(x^2y + \frac{x}{2}y^2\Big)\Big|^{1}_{0} \\
      &= \frac{12}{7} \Big(x^2 + \frac{x}{2}\Big)
      \end{align*}
    \end{exampleblock}
\end{frame}

\begin{frame}{More than two random variables}
  \begin{itemize}
    \item For several jointly continuous random variables, we can make the obvious generalizations.
    \item That is, to find the \emph{marginal} densities, we need to ``marginalize-" or ``integrate-" out the \alert{nusaince} variables.
    \item This means integrating out any combination of variables that we want.
    \item Example: Let $X$, $Y$, and $Z$ be jointly continuous RVs with pdf $f(x, y, z)$. Then the two-dimensional marginal distribution of $X$ and $Z$ is: 
    $$
    f_{XZ}(x, z) = \int_{-\infty}^\infty f(x, y, z) dy.
    $$
  \end{itemize}
\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version \Sexpr{getRversion()}.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.9]{References and Acknowledgements}

\acknowledgments

\framebreak

\bibliography{../bib4450}

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







