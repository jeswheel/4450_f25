\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{5}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Limit theorems}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]




\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Convergence Concepts}

\begin{frame}[allowframebreaks]{Introduction}
  \begin{itemize}
    \item This material comes primarily from \citet[][Chapter~5]{rice07}, but will be supplemented with material from \citet[][Chapter~5]{casella24}.
    \item In this chapter, we are interested in the convergence of sequences of random variables.
    \item In particular, we are interested in the convergence of the sample mean, $\bar{X}_n = (X_1 + X_2 + \ldots + X_n) / n$, as the number of samples $n$ grows.
    \item Because $\bar{X}_n$ is itself a random variable, we have to carefully define what it means for the convergence of a random variable.
    \item In this class, we are mainly concerned with three types of convergence.
    \item Because convergence of random variables is a tricky topic, we will treat them in varying amounts of detail.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Convergence in Probability}
  
  \begin{itemize}
    \item The first type of convergence is one of the weaker types, and is usually easy(ish) to verify.

  \begin{block}{Definition: Convergence in Probability}
    A sequence of random variables $X_1, X_2, \ldots$ \alert{converges in probability} to a random variable $X$ if, for every $\epsilon > 0$,
    $$
    \lim_{n \rightarrow \infty} P\big(|X_n - X| \geq \epsilon \big) = 0
    $$
    or, equivalently,
    $$
    \lim_{n \rightarrow \infty} P\big(|X_n - X| < \epsilon\big)= 1.
    $$
  \end{block}
    
    \item We often use the shorthand $X_n \Plim X$ to denote ``$X_n$ converges in probability to $X$ as $n$ goes to infinity". 
    \item Note that the $X_i$ in the definition above do \emph{not} need to be independent and identically distributed.
    \item The distribution of $X_n$ changes as the subscript changes, and each of the convergence concepts we will discuss will describe different ways in which the distribution of $X_n$ converges to some limiting distribution as the subscript becomes large.
    \item A special case is when the limiting random variable $X$ is a constant.
  \end{itemize}
  
  \begin{block}{Example: The (Weak) Law of Large Numbers}
    Let $X_1, X_2, \ldots$ be iid random variables with $E[X_i] = \mu$ and $\Var(X_i) = \sigma^2$.
    Define $\bar{X}_n = (1/n)\sum^{n}_{i = 1} X_i$. Then $\bar{X}_n \Plim \mu$.
    
    \mode<article>{
    \begin{proof}
    
      The proof is a straightforward application of Chebychev's Inequality.
        
      \begin{itemize}
        \item We want to show that 
        $$\lim_{n \rightarrow \infty} P\big(|\bar{X}_n - \mu| \geq \epsilon \big) = 0.$$ 
        \item For every $\epsilon > 0$, Chebychev's inequality gives us:
        \begin{align*}
        P\big(|\bar{X}_n - \mu| \geq \epsilon \big) &= P\big((\bar{X}_n - \mu)^2 \geq \epsilon^2\big) \\
        &\leq \frac{E\big[(\bar{X}_n - \mu)^2\big]}{\epsilon^2} \\
        &= \frac{\Var(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2}.
        \end{align*}
        \item Thus, taking the limit, we have $\lim_{n \rightarrow \infty} P\big(|\bar{X}_n - \mu| \geq \epsilon \big) = 0$
      \end{itemize}
    \end{proof}
    }
    
  \end{block}
  
  \mode<presentation>{
  \emph{Proof.}
  }
  
  \framebreak
  
  \begin{itemize}
    \item The WLLN is very elegant; under general conditions, the sample mean of independent random variables approaches the population mean as $n \rightarrow \infty$.
    \item This is also used for proportions, as proportions are just means of indicator random variables.
    \item The WLLN can also be extended to show that the results hold even if the variance is infinite, the only condition needed is that the expectation is finite. However, the proof in this case is beyond the scope of this course.
    \item When a sequence of the ``same" sample quantity approaches a constant, we say that the sample quantity is \emph{consistent}.
  \end{itemize}
  
  \framebreak
  
  % \begin{exampleblock}{Example: Consistency of sample variance}
  %   Suppose $X_1, X_2, \ldots$ are sequence of iid random variables with $E[X_i] = \mu$ and $\Var(X_i) = \sigma^2 < \infty$. Define the sample variance as
  %   $$
  %   S_n^2 = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X}_n)^2.
  %   $$
  %   Show that $S_n^2 \Plim \sigma^2$.
  %   
  %   \mode<article>{
  %   \begin{proof}
  %     Once again, we will use the Chebychev inequality. We have
  %     $$
  %     P\big(|S_n^2 - \sigma^2| \geq \epsilon\big) \leq \frac{E\big[(S_n^2 - \sigma^2)^2\big]}{\epsilon^2} = \frac{\Var (S_n^2)}{\epsilon^2}.
  %     $$
  %     Thus, a sufficient condition that $S_n^2$ converges in probability to $\sigma^2$ is that $\Var(S_n^2) \rightarrow 0$ as $n \rightarrow \infty$. 
  %   \end{proof}
  %   }
  % \end{exampleblock}
  % 
  % \mode<presentation>{
  % \emph{Proof.}
  % }
  
  \begin{itemize}
    \item A natural extension of the definition of the convergence of probability, is convergence of functions of random variables: $h(X_1), h(X_2), \ldots$. 
  \end{itemize}
  
  \begin{block}{Theorem: Convergence in probability for continuous functions}
    Let $X_1, X_2, \ldots$ be a sequence of random variables that converges in probability to a random variable $X$, and let $h$ be a continuous function.
    
    Then, $h(X_1), h(X_2), \ldots$ converges in probability to $h(X)$. 
  \end{block}
  
\end{frame}

\begin{frame}[allowframebreaks]{Almost sure convergence}
  \begin{itemize}
    \item Our next convergence concept is stronger than convergence in probability.
    
    \begin{block}{Definition: Almost Sure Convergence}
      A sequence of random variables $X_1, X_2, \ldots$ converge \alert{almost surely} to a random variable $X$ if, for every $\epsilon > 0$,
      $$
      P\big(\lim_{n\rightarrow \infty} |X_n - X| < \epsilon \big) = 1,
      $$
      or
      $$
      P\big(\lim_{n \rightarrow \infty} X_n = X\big) = 1. 
      $$
    \end{block}
    
    \item Almost sure convergence is often written as $X_n \overset{a.s.}{\rightarrow} X$.
    \item It appears similar to convergence in probability, but they are in fact very different. In particular, almost sure convergence is a stronger concept.
    \item One way to think about this difference is that the probability gives a weight to individual sets.
    \item For convergence in probability, the set where $|X_n - X| > \epsilon$ can have positive probability, but that probability converges to zero for large $n$. 
    \item For almost sure convergence, the set where $|X_n - X| > \epsilon$ has probability zero. This doesn't imply that the set $|X_n - X| > \epsilon$ is empty, but it has zero probability.
    \item Almost sure convergence is very similar to pointwise convergence of a sequence of functions. This is no accident, as random variables \emph{are} functions:
    $$
    P\big(\omega \in \Omega: \lim_{n\rightarrow \infty} X_n(\omega) = X(\omega)\big) = 1.
    $$
    \item In the equivalent definition above, we see we must have point-wise convergence \alert{almost-everywhere}, except for the possibility that for some set $N \subset \Omega$ such that $P(N) = 0$, we allow $s \in N$ to not converge: $\lim_{n\rightarrow \infty} X_n(s) \neq X(s)$. 
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Example: Convergence in prob, not a.s.}
    Let the sample space $\Omega = [0, 1]$, and assign the uniform probability on this interval. Define the sequence of random variables $X_i$ as:
    % \begin{align*}
      $X_1(s) = s + 1_{[0, 1]}(s), \, X_2(s) = s + 1_{[0, \frac{1}{2}]}(s),\, X_3(s) = s + 1_{[\frac{1}{2}, 1]}(s), 
      X_4(s) = s + 1_{[0, \frac{1}{3}]}(s),\, X_5(s) = s + 1_{[\frac{1}{3}, \frac{2}{3}]}(s),\, X_6(s) = s + 1_{[\frac{2}{3}, 1]}(s), \ldots$, and then define $X(s) = s$. We can see that $X_n \Plim X$. However, $X_n$ does not converge almost surely, because there is \alert{no} values $s \in \Omega$ that satisfy $X_n(s) \rightarrow X(s)$. For every $\omega$, the value of $X_n(s)$ alternates between $s$ and $s + 1$ infinitely often. 
    % \end{align*}
  \end{exampleblock}
  
  \framebreak
  
  \begin{block}{Theorem: almost sure convergence implies convergence in probability}
    If $X_1, X_2, \ldots$ are a sequence of random variables such that $X_n \overset{a.s.}{\rightarrow} X$, for some random variable $X$, then $X_n \Plim X$.
  \end{block}
  
  \begin{itemize}
    \item The converse of the statement above is false. That is, convergence in probability does not imply almost sure convergence.
    \item A proof of the theorem above, as well as additional treatment of the connection between almost sure convergence and convergence in probability is found in \citet[][Chatper~6]{resnick19}.
    \item Note: As stated, the weak-law of large numbers (WLLN) can actually be shown to hold a.s., in which case we call it the strong-law of large numbers (SLLN).
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Convergence in Distribution}
  \begin{itemize}
    \item The final form of convergence we will consider in this course is convergence in distribution.
  \end{itemize}
  
  \begin{block}{Definition: Convergence in Distribution}
    A sequence of random variables $X_1, X_2, \ldots$ \alert{converges in distribution} to a random variable $X$ if
    $$
    \lim_{n \rightarrow \infty} F_{X_n}(x) = F_X(x)
    $$
    at all points $x$ where $F_X(x)$ is continuous.
  \end{block}
  
  \begin{itemize}
    \item One way to think about convergence in distribution is that it's really a statement about the long-run behavior of a sequence of random variables, as it's a statement about the CDFs. 
    \item This is different from the other types of convergence, which are concerned with the random variable itself.
    \item A quick recap of how the different types of convergence are related:
    \begin{itemize}
      \item a.s. convergence $\implies$ convergence in prob $\implies$ convergence in Distribution.
    \end{itemize}
    \item In a \emph{few} special scenarios, we can talk about more connections between the types of convergence.
    \item One such example is convergence in probability to a constant. \citet[Theorem~5.5.13 of][]{casella24} shows that $X_n \Plim a$ for some constant $a$ if and only if $X_n \dlim a$. 
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{The Central Limit Theorem}
  \begin{itemize}
    \item TODO: Add continuity theorem (Rice CH 5.3)
    \item Add some primer about the CLT
    \item Prove the CLT. 
  \end{itemize}
\end{frame}

% TODO: Add Slutsky's theorem
% TODO: Add Delta Method (1st and 2nd order)

\section{Test Section}

\begin{frame}[allowframebreaks]{Test Frame}

\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version 4.5.1.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}
  
\bibliography{../bib4450}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







