\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{6}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Distributions Derived from the Normal Distribution}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]




\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{$\chi^2$ distributions}

\begin{frame}[allowframebreaks]{Introduction}
  \begin{itemize}
    \item This material comes primarily from \citet[][Chapter~6]{rice07}.
    \item Here, we introduce several important distributions that arise from transformations applied to normal distributions.
    \item Many of these distributions form the basis of traditional statistical inference procedures that are taught in introductory statistics courses.
    \item They are very useful in practice due to the central limit theorem: with enough observations, the limiting behavior of nearly all distributions is normal, so distributions that come from the normal distribution arise in practice as well.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{$\chi_\nu^2$ Distribution}
  \begin{itemize}
    \item The first distribution we will consider is the $\chi^2_1$ (Chi-square with 1 degree of freedom).
  \end{itemize}
  \begin{block}{Definition: $\chi^2_1$ distribution}
    If $Z$ is a standard normal random variable, then $X = Z^2$ is called the chi-square distribution with $1$ degree of freedom.
  \end{block}
  \begin{itemize}
    \item We typically use the notation $X \sim \chi^2_{1}$ (in LaTeX: \texttt{\textbackslash \!chi}).
  \end{itemize}
  \framebreak
  \begin{exampleblock}{The pdf of $\chi^2_1$}
    Let $X$ follow a $\chi^2_1$ distribution. Then, the pdf of $X$ is given by
    $$
    f_X(x) = \frac{1}{\sqrt{2\pi}}x^{-1/2}e^{-x/2}.  
    $$
    
    \mode<article>{
    \begin{proof}
    There are a few ways to show this is the case, and was one of the early examples we saw in Chapter 2. For practice, we repeat this example here.
    \begin{itemize}
      \item By definition, $X$ has the same distribution of $Z^2$, where $Z$ is a standard normal.
      \item Recall the standard normal density is:
      $$
      \phi(z) = \frac{1}{\sqrt{2\pi}}e^{-z^2/2}
      $$
      \item Using the CDF method, we write
      \begin{align*}
        F_X(x) &= P(X \leq x) \\
        &= P(Z^2 \leq x) \\
        &= P(-\sqrt{x} \leq Z \leq \sqrt{x}) \\
        &= P(Z \leq \sqrt{x}) - P(Z \leq -\sqrt{x}) \\
        &= \Phi(\sqrt{x}) - \Phi(-\sqrt{x}),
      \end{align*}
      \item Where $\Phi(z)$ is the cdf of $Z$.
      \item Taking the derivative of both sides of the equation, the chain rule gives us
      \begin{align*}
        f_X(x) &= \frac{1}{2}x^{-1/2}\phi(\sqrt{x}) + \frac{1}{2}x^{-1/2}\phi(-\sqrt{x}) \\
        &= x^{-1/2}\phi(\sqrt{x}),
      \end{align*}
      \item where the last step is a result of the symmetry of $\phi(x)$, noting $\phi(-x) = \phi(x)$ for all $x \in \R$.
      \item Thus, replacing $\phi(\sqrt{x})$ with the definition,
      $$
      f_X(x) = \frac{1}{\sqrt{2\pi}}x^{-1/2}e^{-x/2}
      $$
    \end{itemize}
    
    \end{proof}
    }
  \end{exampleblock}
  \framebreak
  \begin{itemize}
    \item In Chapter~2, we previously noted that that $f_X(x)$ is an example of a Gamma distribution.
    \item Specifically, the \emph{kernel} of the Gamma density is $x$ raised to some power, and $e$ raised to some multiple of $x$: 
    $$
    f_{\text{Gamma}}(x) \propto x^{\alpha - 1}e^{-\lambda x}.
    $$
    \item Thus, ignoring the constant for a moment, if $\alpha = 1/2$, $\lambda = 1/2$, then the pdf of $X \sim \chi^2_1$ is just this Gamma density: 
    $$
    f_{X}(x) \propto x^{-1/2}e^{-x/2} = x^{\alpha - 1}e^{-\lambda x}.
    $$
    \item Since both functions are proper probability density functions, they have to integrate to one, so the normalizing constant \emph{must} be the same.
    \item This is also easily verified. The normalizing constant of the Gamma distribution is $\lambda^\alpha / \Gamma(\alpha)$.
    \item With our specific values of $\lambda = \alpha = 1/2$, and recalling that $\Gamma(1/2) = \sqrt{\pi}$, 
    $$
    \frac{1}{\sqrt{2\pi}} = \frac{(1/2)^{(1/2)}}{\Gamma(1/2)} = \frac{\lambda^\alpha}{\Gamma(\alpha)}
    $$
  \end{itemize}
  \begin{alertblock}{MGF of $\chi^2_1$}
    We previously derived the MGF of a Gamma$(\alpha, \lambda)$ distribution: $M(t) = \big(\lambda / (\lambda - t)\big)^\alpha$. Thus, the MGF of a Chi-square(1) distribution is
    $$
    M(t) = (1 - 2t)^{-1/2}, \quad t < 1/2.
    $$
  \end{alertblock}
  
  \framebreak
  
  \begin{block}{Definition}
    If $U_1, U_2, \ldots, U_n$ are $n$ independent $\chi^2_1$ random variables, then
    $$
    V = U_1 + U_2 + \ldots + U_n
    $$
    then the distribution of $V$ is called the Chi-square distribution with $n$ degrees of freedom, denoted $\chi^2_n$. 
  \end{block}
  
  \begin{itemize}
    \item There are a few different ways of deriving the pdf of a $\chi^2_n$ random variable. Here, we will use the MGF uniqueness theorem.
    \item Let $M_i(t)$ denote the MGF of $U_i$, where $U_i \sim \chi^2_1$. Then, due to independence,
    \begin{align*}
      M_V(t) = M_{\sum_{i} U_i}(t) = \prod_{i = 1}^n M_i(t) = \big(M_t(t)\big)^n = (1 - 2t)^{-n/2}
    \end{align*}
    \item Compare this to the Gamma MGF: $M(t) = \big(\lambda / (\lambda - t)\big)^\alpha$. Then, setting $\lambda = 1/2$, $\alpha = n/2$, we see that $V$ has a Gamma$(n/2, 1/2)$ distribution.
    \item Thus, the pdf of $V$ is given by: 
    $$
    f_V(v) = \frac{1}{2^{n/2}\Gamma(n/2)}v^{(n/2) - 1}e^{-v/2}.
    $$
    \item The expected value and variance of the $\chi^2_n$ distribution can easily be found then by using the fact that it is a special case of a Gamma distribution.
  \end{itemize}
  
\end{frame}

\section{The $t$ and $F$ distributions}

\begin{frame}[allowframebreaks]{The Student's $t$ distributions}
  \begin{block}{The Student's $t$ distribution}
    If $Z \sim N(0, 1)$ and $U \sim \chi^2_n$, and $Z$ and $U$ are independent, then the distribution of $T$, where
    $$
    T = \frac{Z}{\sqrt{U / n}},
    $$
    is called the Student's $t$ distribution (or simply the $t$ distribution) with $n$ degrees of freedom, which is often denoted $t_{n}$
  \end{block}
  
  \begin{itemize}
    \item Students often forget the make sure that $Z$ and $U$ in the definition of the $t$ distribution are independent.
    \item The $t$ distribution is the distribution used to perform the famed ``$t$-test".
  \end{itemize}
  
  \framebreak
  
  \begin{alertblock}{The density of the $t_n$ distribution}
    The pdf of the $t$ distribution with $n$ degrees of freedom is:
    $$
    f(t) = \frac{\Gamma\big((n + 1) / 2\big)}{\sqrt{n\pi}\Gamma(n/2)}\Big(1 + \frac{t^2}{n}\Big)^{-(n+1)/2}
    $$
  \end{alertblock}
  
  \begin{itemize}
    \item The derivation of the pdf of a $t$ distribution is a good practice exercise.
    \item Recall it is defined as the ratio of two independent random variables; in Chapter~3, we derived a formula for computing densities of random variables of this form.
    \item Note that $f(t) = f(-t)$, and so $f$ is symmetric about zero.
    \item It also has a bell-curve shape similar to a normal distribution.
    \framebreak
    \item You can see as $n\rightarrow \infty$, the $t_n$ distribution converges to the standard normal (e.g., use Slutsky's theorem, good practice).
  \end{itemize}
  
\begin{figure}[ht]
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{tmp/figure/tDistFig-1} 

}


\end{knitrout}
\end{figure}
 
  
\end{frame}

% TODO: Add examples of CLT (statistical practice), Delta method, and second-order delta-method.

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version 4.5.2.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}
  
\bibliography{../bib4450}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







